{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from shapely import wkt\n",
    "\n",
    "from his_geo import extractor\n",
    "from his_geo import geocoder\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "from ckip_transformers.nlp import CkipNerChunker\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closeness(x, n):\n",
    "    if x <= n:\n",
    "        return (np.cos(x*(np.pi/n))+1)/2\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def shapely_point_to_geopy(point_shapely):\n",
    "    return (point_shapely.y, point_shapely.x)\n",
    "\n",
    "\n",
    "def calculate_geo_closeness(point1, point2, maximum_error_distance):\n",
    "    # Geodesic Distance between two points\n",
    "    point1, point2 = shapely_point_to_geopy(point1), shapely_point_to_geopy(point2)\n",
    "    distance =  geodesic(point1, point2).kilometers\n",
    "    # Maximum error distance as cut-off\n",
    "    n = maximum_error_distance\n",
    "    # Closeness\n",
    "    closeness_value = closeness(distance, n)\n",
    "    return closeness_value\n",
    "\n",
    "\n",
    "def calculate_geo_closeness_for_all_text(gdf_result, gdf_benchmark):\n",
    "    # # Check if the number of unique ids in the result is the same as the benchmark\n",
    "    # if len(gdf_result['id'].unique()) != len(gdf_benchmark['id'].unique()):\n",
    "    #     print(\"Number of unique ids in the result is not the same as the benchmark\")\n",
    "    # else:\n",
    "    gdf_result['closeness'] = 0.0\n",
    "    gdf_result['target_toponym'] = None\n",
    "    # Iterate through each text (section)\n",
    "    for id in gdf_benchmark['id'].unique():\n",
    "        gdf_section_benchmark = gdf_benchmark[gdf_benchmark['id'] == id]\n",
    "        gdf_section_result = gdf_result[gdf_result['id'] == id]\n",
    "\n",
    "        # Iterate through each extracted toponym\n",
    "        for index_result, row_result in gdf_section_result.iterrows():\n",
    "            if len(gdf_section_result) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                result_toponym = row_result['toponym'].replace(\"县\", \"\").replace(\"国\", \"\")\n",
    "                for index_benchmark, row_benchmark in gdf_section_benchmark.iterrows():\n",
    "                    benchmark_toponym = row_benchmark['toponym'].replace(\"县\", \"\").replace(\"国\", \"\")\n",
    "                    if benchmark_toponym in result_toponym or result_toponym in benchmark_toponym:\n",
    "                        gdf_result.loc[index_result, 'target_toponym'] = benchmark_toponym\n",
    "                        if row_result['geometry'] is None or row_result['geometry'].is_empty:\n",
    "                            closeness_value = 0\n",
    "                        else:\n",
    "                            if row_benchmark['geometry'] is None or row_benchmark['geometry'].is_empty:\n",
    "                                # Change in the future\n",
    "                                closeness_value = 0\n",
    "                            else:\n",
    "                                # Get the most accurate level polygon of the benchmark data\n",
    "                                closeness_value = calculate_geo_closeness(row_result['geometry'], row_benchmark['geometry'], row_benchmark['Maximum Error Distance'])\n",
    "                                break\n",
    "                    else:\n",
    "                        closeness_value = 0\n",
    "                # Add the closeness score to the result dataframe\n",
    "                gdf_result.loc[index_result, 'closeness'] = closeness_value\n",
    "\n",
    "    return gdf_result\n",
    "\n",
    "\n",
    "def calculate_precision(gdf_result):\n",
    "    section_precisions = gdf_result.groupby('id')['closeness'].mean()\n",
    "    total_precision = section_precisions.mean()\n",
    "    return total_precision\n",
    "\n",
    "\n",
    "def calculate_scores(gdf_result, gdf_benchmark):\n",
    "\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "\n",
    "    for id in gdf_result['id'].unique():\n",
    "        gdf_section_benchmark = gdf_benchmark[gdf_benchmark['id'] == id]\n",
    "        gdf_section_result = gdf_result[gdf_result['id'] == id]\n",
    "\n",
    "        # calculate precision for each section (text)\n",
    "        section_precision = gdf_section_result['closeness'].mean()\n",
    "        total_precision += section_precision\n",
    "\n",
    "        # calculate recall for each section (text)\n",
    "        section_recall = 0\n",
    "        for i in range(len(gdf_section_benchmark)):\n",
    "            benchmark_toponym = gdf_section_benchmark.iloc[i]['toponym'].replace(\"县\", \"\").replace(\"国\", \"\")\n",
    "            if benchmark_toponym not in gdf_section_result['target_toponym'].tolist():\n",
    "                section_recall += 0\n",
    "            else:\n",
    "                section_recall += gdf_section_result[gdf_section_result['target_toponym'] == benchmark_toponym]['closeness'].max()\n",
    "        section_recall = section_recall / len(gdf_section_benchmark)\n",
    "        total_recall += section_recall\n",
    "\n",
    "        # calculate f1 score for each section (text)\n",
    "        if section_precision == 0 and section_recall == 0:\n",
    "            section_f1 = 0\n",
    "        else:\n",
    "            section_f1 = 2 * section_precision * section_recall / (section_precision + section_recall)\n",
    "        total_f1 += section_f1\n",
    "\n",
    "    total_precision = total_precision / len(gdf_result['id'].unique())\n",
    "    total_recall = total_recall / len(gdf_result['id'].unique())\n",
    "    total_f1 = total_f1 / len(gdf_result['id'].unique())\n",
    "   \n",
    "\n",
    "    return total_precision, total_recall, total_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dataframe(json_file):\n",
    "    with open(json_file, 'r', encoding=\"utf-8-sig\") as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(columns=['id', 'toponym', 'location'])\n",
    "    for key, value in data.items():\n",
    "        result = value.split('\\n')\n",
    "        for item in result:\n",
    "            if item != '':\n",
    "                try:\n",
    "                    row = {'id': key, 'toponym': item.split(',')[0].strip(), 'location': item.split(',')[1].strip()}\n",
    "                except:\n",
    "                    row = {'id': key, 'toponym': item, 'location': ''}\n",
    "                df.loc[len(df)] = row\n",
    "    return df\n",
    "\n",
    "\n",
    "def match_ids(df_result, df_original):\n",
    "    df_ids = df_original[['id']].copy()\n",
    "\n",
    "    df_result.set_index('id', inplace=True)\n",
    "    df_result.index.name = None\n",
    "    df_ids.index = df_ids.index.astype('int64')\n",
    "    df_result.index = df_result.index.astype('int64')\n",
    "\n",
    "    df_result = df_result.merge(df_ids, left_index=True, right_index=True, how='left')\n",
    "    df_result = df_result[['id', 'toponym', 'location']]\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gpe_entities(ner_tokens):\n",
    "    # Filter GPE tokens\n",
    "    gpe_tokens = [token for token in ner_tokens if token.ner == 'GPE']\n",
    "    # Sort tokens by their starting index\n",
    "    gpe_tokens.sort(key=lambda token: token.idx[0])\n",
    "    merged_entities = []\n",
    "    if not gpe_tokens:\n",
    "        return merged_entities\n",
    "    # Initialize the first entity\n",
    "    current_entity = gpe_tokens[0].word\n",
    "    current_end = gpe_tokens[0].idx[1]\n",
    "    for token in gpe_tokens[1:]:\n",
    "        start, end = token.idx\n",
    "        if start == current_end:\n",
    "            # Adjacent entity, merge it\n",
    "            current_entity += token.word\n",
    "            current_end = end\n",
    "        else:\n",
    "            # Non-adjacent, add the current entity to the list\n",
    "            merged_entities.append(current_entity)\n",
    "            current_entity = token.word\n",
    "            current_end = end\n",
    "    # Add the last entity\n",
    "    merged_entities.append(current_entity)\n",
    "    return merged_entities\n",
    "\n",
    "\n",
    "def ner_by_ckip(text, model):\n",
    "\n",
    "    ner_driver = CkipNerChunker(model=model)\n",
    "\n",
    "    result = ner_driver([text])[0]\n",
    "\n",
    "    # addresses = [i.word for i in result if i.ner == 'GPE']\n",
    "    addresses = list(set(extract_gpe_entities(result)))\n",
    "    \n",
    "    return addresses\n",
    "\n",
    "\n",
    "def ner_by_cluener(text, model, tokenizer):\n",
    "  ner = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "  tag_type = \"address\"\n",
    "  ner_result = ner(text)\n",
    "  single_name_str = \"\"\n",
    "  name_list = []\n",
    "  for char_dic in ner_result:\n",
    "    # b_or_i, current_entity_type = char_dic['entity'].split(\"-\")\n",
    "    current_entity_type = char_dic['entity'].split(\"-\")[1]\n",
    "    # Start a new entity\n",
    "    if single_name_str == \"\" and (char_dic['entity'] == f\"B-{tag_type}\" or char_dic['entity'] == f\"I-{tag_type}\"):\n",
    "      single_name_str = char_dic['word']\n",
    "    # Concatenate the entity\n",
    "    elif char_dic['entity'] == f\"I-{tag_type}\":\n",
    "      single_name_str+=char_dic['word']\n",
    "    # B-tag type follows another B-tag type\n",
    "    elif char_dic['entity'] == f\"B-{tag_type}\":\n",
    "      name_list.append(single_name_str)\n",
    "      single_name_str = char_dic['word']\n",
    "    # B-tag type ended by a non-type type\n",
    "    elif single_name_str!= \"\" and tag_type!= current_entity_type:\n",
    "      name_list.append(single_name_str)\n",
    "      single_name_str = \"\"\n",
    "# The last captured entity\n",
    "  if single_name_str != \"\":\n",
    "    name_list.append(single_name_str)\n",
    "  # name_list = [i for i in name_list if len(i)>1]\n",
    "  name_list = list(set(name_list))\n",
    "  return name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set prompt\n",
    "prompt = \"\"\"\n",
    "I would like you to take on the roles of both a Geographer and a Historian. \n",
    "You possess extensive knowledge in Chinese geography and history, with a particular expertise in historical toponymy. \n",
    "Your task is to extract precise location references of historical toponyms from texts.\n",
    "When I provide a scholarly text analyzing the location of one or several historical toponyms, please identify and extract both the toponyms and their corresponding location references from the text. \n",
    "Keep the following in mind:\n",
    "1. If the text presents differing opinions of the same historical toponym's location from various scholars, only extract the most correct location reference that the author of the text acknowledges or agrees with. Do not include location references that the author disputes.\n",
    "2. If a toponym is mentioned in the text but no location is provided, please skip this toponym.\n",
    "3. Present the extracted information always in Chinese and strictly adhere to the following format:\n",
    "   \"Toponym 1\", \"Location 1\"\n",
    "   \"Toponym 2\", \"Location 2\"\n",
    "   Please do not include any explanation, verb or extraneous information.\n",
    "\n",
    "The text is as follows:\n",
    "\n",
    "         \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API key\n",
    "api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "# Set models\n",
    "models = [\"chatgpt\"]\n",
    "chatgpt_model_versions = [\n",
    "                          \"gpt-3.5-turbo-0125\",\n",
    "                          \"gpt-4-turbo-2024-04-09\",\n",
    "                          \"gpt-4o-2024-08-06\",\n",
    "                          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/evaluation/raw_text.csv')\n",
    "texts = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create extractor and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text 0 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 1 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 2 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 3 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 4 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 5 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 6 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 7 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 8 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 9 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 10 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 11 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 12 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 13 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 14 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 15 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 16 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 17 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 18 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 19 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 20 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 21 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 22 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 23 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 24 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 25 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 26 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 27 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 28 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 29 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 30 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 31 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 32 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 33 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 34 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 35 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 36 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 37 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 38 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 39 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 40 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 41 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 42 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 43 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 44 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 45 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 46 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 47 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 48 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 49 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 50 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 51 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 52 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 53 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 54 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 55 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 56 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 57 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 58 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 59 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 60 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 61 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 62 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 63 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 64 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 65 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 66 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 67 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 68 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 69 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 70 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 71 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 72 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 73 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 74 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 75 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 76 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 77 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 78 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 79 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 80 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 81 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 82 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 83 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 84 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 85 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 86 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 87 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 88 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 89 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 90 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 91 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 92 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 93 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 94 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 95 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 96 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 97 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 98 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 99 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 100 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 101 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 102 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 103 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 104 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 105 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 106 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 107 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 108 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 109 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 110 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 111 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 112 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 113 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 114 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 115 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 116 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 117 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 118 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 119 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 120 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 121 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 122 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 123 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 124 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 125 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 126 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 127 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 128 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 129 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 130 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 131 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 132 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 133 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 134 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 135 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 136 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 137 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 138 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 139 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 140 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 141 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 142 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 143 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 144 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 145 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 146 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 147 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 148 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 149 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 150 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 151 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 152 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 153 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 154 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 155 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 156 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 157 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 158 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 159 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 160 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 161 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 162 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 163 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 164 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 165 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 166 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 167 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 168 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 169 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 170 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 171 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 172 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 173 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 174 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 175 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 176 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 177 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 178 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 179 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 180 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 181 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 182 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 183 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 184 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 185 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 186 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 187 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 188 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 189 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 190 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 191 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 192 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 193 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 194 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 195 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 196 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 197 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 198 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 199 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 200 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 201 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 202 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 203 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 204 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 205 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 206 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 207 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 208 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 209 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 210 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 211 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 212 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 213 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 214 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 215 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 216 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 217 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 218 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 219 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 220 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 221 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 222 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 223 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 224 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 225 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 226 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 227 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 228 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 229 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 230 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 231 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 232 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 233 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 234 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 235 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 236 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 237 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 238 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 239 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 240 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 241 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 242 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 243 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 244 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 245 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 246 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 247 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 248 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 249 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 250 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 251 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 252 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 253 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 254 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 255 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 256 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 257 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 258 to ./evaluation/extracted_results_chatgpt_gpt-4-turbo-2024-04-09.json\n",
      "Extracting text 0 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 1 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 2 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 3 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 4 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 5 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 6 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 7 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 8 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 9 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 10 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 11 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 12 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 13 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 14 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 15 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 16 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 17 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 18 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 19 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 20 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 21 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 22 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 23 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 24 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 25 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 26 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 27 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 28 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 29 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 30 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 31 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 32 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 33 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 34 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 35 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 36 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 37 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 38 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 39 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 40 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 41 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 42 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 43 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 44 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 45 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 46 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 47 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 48 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 49 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 50 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 51 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 52 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 53 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 54 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 55 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 56 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 57 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 58 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 59 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 60 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 61 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 62 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 63 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 64 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 65 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 66 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 67 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 68 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 69 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 70 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 71 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 72 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 73 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 74 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 75 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 76 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 77 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 78 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 79 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 80 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 81 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 82 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 83 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 84 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 85 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 86 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 87 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 88 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 89 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 90 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 91 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 92 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 93 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 94 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 95 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 96 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 97 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 98 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 99 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 100 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 101 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 102 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 103 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 104 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 105 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 106 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 107 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 108 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 109 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 110 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 111 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 112 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 113 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 114 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 115 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 116 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 117 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 118 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 119 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 120 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 121 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 122 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 123 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 124 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 125 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 126 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 127 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 128 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 129 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 130 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 131 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 132 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 133 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 134 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 135 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 136 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 137 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 138 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 139 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 140 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 141 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 142 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 143 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 144 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 145 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 146 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 147 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 148 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 149 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 150 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 151 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 152 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 153 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 154 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 155 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 156 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 157 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 158 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 159 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 160 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 161 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 162 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 163 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 164 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 165 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 166 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 167 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 168 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 169 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 170 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 171 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 172 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 173 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 174 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 175 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 176 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 177 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 178 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 179 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 180 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 181 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 182 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 183 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 184 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 185 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 186 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 187 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 188 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 189 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 190 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 191 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 192 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 193 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 194 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 195 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 196 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 197 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 198 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 199 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 200 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 201 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 202 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 203 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 204 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 205 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 206 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 207 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 208 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 209 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 210 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 211 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 212 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 213 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 214 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 215 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 216 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 217 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 218 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 219 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 220 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 221 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 222 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 223 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 224 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 225 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 226 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 227 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 228 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 229 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 230 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 231 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 232 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 233 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 234 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 235 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 236 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 237 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 238 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 239 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 240 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 241 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 242 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 243 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 244 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 245 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 246 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 247 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 248 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 249 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 250 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 251 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 252 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 253 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 254 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 255 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 256 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 257 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n",
      "Extracting text 258 to ./evaluation/extracted_results_chatgpt_gpt-4o-2024-08-06.json\n"
     ]
    }
   ],
   "source": [
    "for model in chatgpt_model_versions:\n",
    "\n",
    "    llm_extractor = extractor.Extractor(prompt, output_dir=\"../data/evaluation/\", \n",
    "                                        model=\"chatgpt\", model_version=model, api_key=api_key)\n",
    "\n",
    "    results = llm_extractor.extract_texts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structuralize the results\n",
    "df_original = pd.read_csv('../data/evaluation/raw_text.csv')\n",
    "\n",
    "json_dir = '../data/evaluation/'\n",
    "json_files = [i for i in os.listdir(json_dir) if i.endswith('.json')]\n",
    "\n",
    "for json_file in json_files:\n",
    "    df_result = json_to_dataframe(json_dir + json_file)\n",
    "    df_result = match_ids(df_result, df_original)\n",
    "    df_result.dropna(subset=['location'], inplace=True)\n",
    "    df_result = df_result[~df_result['location'].str.contains('不详')]\n",
    "    df_result = df_result[~df_result['location'].str.contains('未明确')]\n",
    "    df_result = df_result[~df_result['location'].str.contains('未提供')]\n",
    "    df_result = df_result[~df_result['location'].str.contains('不明确')]\n",
    "    df_result = df_result[~df_result['location'].str.contains('不提供')]\n",
    "    df_result = df_result[~df_result['location'].str.contains('未提及')]\n",
    "    df_result = df_result[~df_result['location'].str.contains('未详')]\n",
    "    df_result = df_result[~df_result['location'].str.contains('无法确定')]\n",
    "    df_result = df_result[~df_result['location'].str.contains('[A-Za-z]')]\n",
    "    df_result.to_csv(json_dir + json_file[:-5] + '.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert / Albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.36s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.03s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.98s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 499.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.23s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 996.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 996.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 333.28it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.17s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.16s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 499.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.00s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 996.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1910.84it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 995.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.05s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 996.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.84it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1004.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.20s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 994.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1008.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 994.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 996.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 993.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 986.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 986.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.16s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 996.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 994.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 629.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1004.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 995.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 986.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 994.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 985.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1004.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "100%|██████████| 259/259 [08:10<00:00,  1.89s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.44s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1004.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1007.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.23s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1004.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.03s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 499.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 995.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 986.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.28s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 499.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.56s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 996.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 499.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 991.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.05s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 678.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1974.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1007.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1004.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 995.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 499.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1007.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1004.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.24s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.32s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 658.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 995.09it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.29s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1005.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1907.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 996.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.46s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.57s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.84it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1005.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.46s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1004.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 997.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 947.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 902.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1967.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 995.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 992.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1004.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 996.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s]\n",
      "100%|██████████| 259/259 [05:50<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "df_original = pd.read_csv('../data/evaluation/raw_text.csv')\n",
    "models = [\"bert-base\", 'albert-base']\n",
    "\n",
    "for model in models:\n",
    "    df_original['location'] = df_original['text'].progress_apply(lambda x: ner_by_ckip(x, model))\n",
    "\n",
    "    df_extracted = df_original['location'].explode().reset_index() \n",
    "    merged_df = df_extracted.merge(df_original[['id']], left_on='index', right_index=True)\n",
    "    merged_df = merged_df[['id', 'location']]\n",
    "    merged_df.to_csv('../data/evaluation/' + 'extracted_results_' + model + '.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259/259 [02:18<00:00,  1.87it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('uer/roberta-base-finetuned-cluener2020-chinese')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('uer/roberta-base-finetuned-cluener2020-chinese', model_max_length=512)\n",
    "\n",
    "df_original = pd.read_csv('../data/evaluation/raw_text.csv')\n",
    "df_original['location'] = df_original['text'].progress_apply(lambda x: ner_by_cluener(x, model, tokenizer))\n",
    "\n",
    "df_extracted = df_original['location'].explode().reset_index() \n",
    "merged_df = df_extracted.merge(df_original[['id']], left_on='index', right_index=True)\n",
    "merged_df = merged_df[['id', 'location']]\n",
    "merged_df.to_csv('../data/evaluation/' + 'extracted_results_roberta-cluener.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir = '../data/evaluation/'\n",
    "files = [i for i in os.listdir(files_dir) if \"extracted_results\" in i and i.endswith('.csv')]\n",
    "for file in files:\n",
    "    print(file)\n",
    "    df = pd.read_csv(files_dir + file, encoding='utf-8-sig')\n",
    "    addresses = df['location'].tolist()\n",
    "    geocoder_evaluate = geocoder.Geocoder(addresses, \n",
    "                                          lang=\"ch\", \n",
    "                                          preferences=['modern', 'historic'], \n",
    "                                          geographic_crs=\"EPSG:4326\", \n",
    "                                          if_certainty=True)\n",
    "    geocoder_evaluate.detect_direction()\n",
    "    geocoder_evaluate.match_address()\n",
    "    geocoder_evaluate.calculate_point()\n",
    "    df_geocoded = geocoder_evaluate.data.reset_index().copy().drop(columns=['id'])\n",
    "    df = df.reset_index().copy()\n",
    "    df = df.merge(df_geocoded, left_index=True, right_index=True, how='left')\n",
    "    df.to_csv(files_dir + file[:-4] + '_geocoded.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted_results_albert-base_geocoded.csv\n",
      "extracted_results_bert-base_geocoded.csv\n",
      "extracted_results_chatgpt_gpt-3.5-turbo-0125_geocoded.csv\n",
      "extracted_results_chatgpt_gpt-4-turbo-2024-04-09_geocoded.csv\n",
      "extracted_results_chatgpt_gpt-4o-2024-08-06_geocoded.csv\n",
      "extracted_results_roberta-cluener_geocoded.csv\n"
     ]
    }
   ],
   "source": [
    "file_dir = '../data/evaluation/'\n",
    "files = [i for i in os.listdir(file_dir) if 'extracted_results' in i and i.endswith('geocoded.csv')]\n",
    "\n",
    "df_benchmark = pd.read_csv('../data/evaluation/benchmark.csv', encoding='utf-8-sig')\n",
    "gdf_benchmark = gpd.GeoDataFrame(df_benchmark, geometry=gpd.points_from_xy(df_benchmark.X, df_benchmark.Y))\n",
    "\n",
    "for file in files:\n",
    "    print(file)\n",
    "    df_result = pd.read_csv(file_dir + file, encoding='utf-8-sig')\n",
    "    df_result = df_result.dropna(subset=['geometry'])\n",
    "    df_result['geometry'] = df_result['geometry'].astype(str)\n",
    "    df_result['geometry'] = df_result['geometry'].apply(wkt.loads)\n",
    "\n",
    "    gdf_result = gpd.GeoDataFrame(df_result, geometry='geometry')\n",
    "    if \"toponym\" not in gdf_result.columns:\n",
    "        gdf_result = gdf_result.merge(gdf_benchmark[['toponym', 'id']], on=\"id\", how=\"left\")\n",
    "    gdf_result = calculate_geo_closeness_for_all_text(gdf_result, gdf_benchmark)\n",
    "    gdf_result.to_csv(file_dir + file[:-4] + '_evaluated.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted_results_albert-base_geocoded_evaluated.csv\n",
      "Precision: 0.3992463007425532\n",
      "Recall: 0.7319299157136846\n",
      "F1: 0.48620597072648497\n",
      "extracted_results_bert-base_geocoded_evaluated.csv\n",
      "Precision: 0.40934062335900817\n",
      "Recall: 0.7447511615696952\n",
      "F1: 0.4938614115857279\n",
      "extracted_results_chatgpt_gpt-3.5-turbo-0125_geocoded_evaluated.csv\n",
      "Precision: 0.6836116620288818\n",
      "Recall: 0.7846726882937356\n",
      "F1: 0.708676986462774\n",
      "extracted_results_chatgpt_gpt-4-turbo-2024-04-09_geocoded_evaluated.csv\n",
      "Precision: 0.7334322675403984\n",
      "Recall: 0.8106123398741139\n",
      "F1: 0.7559727840874089\n",
      "extracted_results_chatgpt_gpt-4o-2024-08-06_geocoded_evaluated.csv\n",
      "Precision: 0.8294277564378774\n",
      "Recall: 0.8478268665902011\n",
      "F1: 0.8305851796084152\n",
      "extracted_results_roberta-cluener_geocoded_evaluated.csv\n",
      "Precision: 0.5478507527556477\n",
      "Recall: 0.9143821019914481\n",
      "F1: 0.6441279041830332\n"
     ]
    }
   ],
   "source": [
    "file_dir = '../data/evaluation/'\n",
    "files = [i for i in os.listdir(file_dir) if 'extracted_results' in i and i.endswith('evaluated.csv')]\n",
    "for file in files:\n",
    "    df = pd.read_csv(file_dir + file, encoding='utf-8-sig')\n",
    "\n",
    "    # remove results with no locations\n",
    "    # df.dropna(subset=['location'], inplace=True)\n",
    "\n",
    "    precision, recall, f1 = calculate_scores(df, gdf_benchmark)\n",
    "    print(file)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1:', f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
